""" Utils for io, language, connectivity graphs etc """

import sys
import json
import numpy as np
import networkx as nx
import torch

import nltk
nltk.download('stopwords')

from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords

# padding, unknown word, end of sentence
base_vocab = ['<PAD>', '<UNK>', '<EOS>']
padding_idx = base_vocab.index('<PAD>')


def load_nav_graphs(scans):
    """ Load connectivity graph for each scan """

    def distance(pose1, pose2):
        """ Euclidean distance between two graph poses """
        return ((pose1['pose'][3] - pose2['pose'][3]) ** 2
                + (pose1['pose'][7] - pose2['pose'][7]) ** 2
                + (pose1['pose'][11] - pose2['pose'][11]) ** 2) ** 0.5

    graphs = {}
    for scan in scans:
        with open('connectivity/%s_connectivity.json' % scan) as f:
            g = nx.Graph()
            positions = {}
            data = json.load(f)
            for i, item in enumerate(data):
                if item['included']:
                    for j, conn in enumerate(item['unobstructed']):
                        if conn and data[j]['included']:
                            positions[item['image_id']] = np.array([item['pose'][3],
                                                                    item['pose'][7], item['pose'][11]]);
                            assert data[j]['unobstructed'][i], 'Graph should be undirected'
                            g.add_edge(item['image_id'], data[j]['image_id'], weight=distance(item, data[j]))
            nx.set_node_attributes(g, values=positions, name='position')
            graphs[scan] = g
    return graphs


def load_datasets(splits):
    data = []
    for split in splits:
        assert split in ['train', 'val_seen', 'val_unseen', 'test', 'synthetic', 'R4R_train', 'R4R_val_seen', 'R4R_val_unseen']
        if split in ['train', 'val_seen', 'val_unseen', 'test', 'synthetic']:
            filename = 'tasks/R2R/data/R2R_%s.json' % split
        elif split in ['R4R_train', 'R4R_val_seen', 'R4R_val_unseen']:
            filename = 'tasks/R2R/data/%s.json' % split
        else:
            raise KeyError
        with open(filename) as f:
            data += json.load(f)
    return data


def print_progress(iteration, total, prefix='', suffix='', decimals=1, bar_length=100):
    """
    Call in a loop to create terminal progress bar
    @params:
        iteration   - Required  : current iteration (Int)
        total       - Required  : total iterations (Int)
        prefix      - Optional  : prefix string (Str)
        suffix      - Optional  : suffix string (Str)
        decimals    - Optional  : positive number of decimals in percent complete (Int)
        bar_length  - Optional  : character length of bar (Int)
    """
    str_format = "{0:." + str(decimals) + "f}"
    percents = str_format.format(100 * (iteration / float(total)))
    filled_length = int(round(bar_length * iteration / float(total)))
    bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)

    sys.stdout.write('\r%s |%s| %s%s %s' % (prefix, bar, percents, '%', suffix)),

    if iteration == total:
        sys.stdout.write('\n')
    sys.stdout.flush()


def check_config_trainer(config):
    field_list = ['action_space', 'features', 'img_spec', 'splits', 'batch_size', 'seed', 'results_path']
    assert isinstance(config, dict), 'expected type dict for argument config, found %s' % type(config)
    for field in field_list:
        assert field in config, 'missing required field in config: %s' % field
    assert config['action_space'] in ['low', 'high'], 'action space should be either "low" or "high", found %s' % config['action_space']
    return config


def check_config_judge(config):
    field_list = ['action_space', 'features', 'img_spec', 'splits', 'batch_size', 'seed', 'results_path']
    assert isinstance(config, dict), 'expected type dict for argument config, found %s' % type(config)
    for field in field_list:
        assert field in config, 'missing required field in config: %s' % field
    assert config['action_space'] in ['low', 'high'], 'action space should be either "low" or "high", found %s' % config['action_space']

    if isinstance(config['splits'], str):
        config['splits'] = [config['splits']]
    assert isinstance(config['splits'], list), 'expected type list or str type for argument "splits", found %s' % type(config['splits'])

    return config


def my_split_func(sentence):
    tokenizer = RegexpTokenizer(r'\w+')
    stop_words = set(stopwords.words('english'))
    include_words = ['up', 'down', 'out', 'into', 'through', 'above', 'before', 'below', 'to', 'over', 'under']

    for word in include_words:
        stop_words.remove(word)

    word_tokens = tokenizer.tokenize(sentence)
    filtered_sentence = [w for w in word_tokens if w not in stop_words]
    filtered_sentence_reversed = [w for w in reversed(word_tokens) if w not in stop_words]

    return filtered_sentence, filtered_sentence_reversed


def batched_sentence_embedding(batch, word_encoder, device=torch.device('cpu')):
    """
    :param batch: batch of instructions of variable lengths --> suppose range [min_l, max_l]
    :param word_encoder: provides single-word embeddings --- must support __getitem__ method
    :param device: may be cpu or cuda -- default is cpu
    :return: tensor of shape [batch_len, max_l, embedding_size] where sentences are zero-padded to have same size
    """
    split_batch = []

    for sentence in batch:
        spl, spl_rev = my_split_func(sentence)
        split_batch.append(spl)

    lengths = [len(spl) for spl in split_batch]
    max_l = max(lengths)

    t = torch.zeros(len(batch), max_l, 300)

    for i, spl in enumerate(split_batch):
        e = torch.stack([word_encoder[word] for word in spl])
        t[i, :e.shape[0], :] = e.squeeze(dim=1)

    t = t.transpose(1, 2)

    embeddings = t.to(device=device)
    return embeddings, lengths


def append_coordinates(features, agent_heading, agent_elevation):
    """ Appends elevation and headings coordinates to attention heatmap """

    """
    Assume features is 36 x num_features: appends 36-dimensional maps with elevation and headings.
    Indexing is the following:
            _________________________________________________
            |                                                |
    up      | 24  25  26  27  28  29  30  31  32  33  34  35 |
            |                                                |
    center  | 12  13  14  15  16  17  18  19  20  21  22  23 |
            |                                                |
    down    |  0   1   2   3   4   5   6   7   8   9  10  11 |
            |________________________________________________|

            left                 center                  right
    """

    abs_elevations = torch.tensor([-0.5, 0, 0.5], dtype=torch.float)
    elevations = abs_elevations - agent_elevation
    elevations_map = elevations.repeat(12, 1).transpose(0, 1).contiguous().view(36, 1)

    abs_headings = torch.tensor(np.linspace(0, (11./6.)*np.pi, 12), dtype=torch.float)
    headings = abs_headings - agent_heading

    headings_cos_map = torch.cos(headings).repeat(3).view(36, 1)
    headings_sin_map = torch.sin(headings).repeat(3).view(36, 1)

    feature_map = torch.cat((features, elevations_map, headings_cos_map, headings_sin_map), dim=-1)

    return feature_map


def to_one_hot(indexes, output_dim):
    """
    :param indexes: list of numbers in the range [0, output_dim)
    :param output_dim: size of a single one-hot tensor
    :return: tensor containing one_hot representation of indexes
    """
    assert output_dim >= 2
    assert output_dim > max(indexes)
    assert min(indexes) >= 0

    return torch.eye(output_dim)[indexes]


def position_embedding(input, d_model):
    input = input.view(-1, 1)
    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)
    sin = torch.sin(input / 10000 ** (2 * dim / d_model))
    cos = torch.cos(input / 10000 ** (2 * dim / d_model))

    out = torch.zeros((input.shape[0], d_model), device=input.device)
    out[:, ::2] = sin
    out[:, 1::2] = cos
    return out


def sinusoid_encoding_table(max_len, d_model, padding_idx=None):
    pos = torch.arange(max_len, dtype=torch.float32)
    out = position_embedding(pos, d_model)

    if padding_idx is not None:
        out[padding_idx] = 0
    return out
